{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Answer1-Convolution of Two Arrays**\n",
    "## **1. Given Arrays**\n",
    "$ A = [3, 4, 5, 6, 2]$\n",
    "$ B = [1, 2, 3]$\n",
    "---\n",
    "## **2. Convolution Calculation**\n",
    "\n",
    "### **2.1 Formula**\n",
    "The discrete convolution  $C$ of two arrays $A$ and $B$ is given by:\n",
    "$C[n] = (sum_{k=0}^{K-1} A[k] \\cdot B[n - k]$\n",
    "\n",
    "---\n",
    "### *Calculation*\n",
    "1. **$C[0]$**\n",
    "   $C[0] = A[0] \\cdot B[0] = 3 \\cdot 1 = 3$\n",
    "2. **$C[1]$**\n",
    "   $C[1] = A[0] \\cdot B[1] + A[1] \\cdot B[0] = 3 \\cdot 2 + 4 \\cdot 1 = 6 + 4 = 10$\n",
    "3.  **$C[2]$**\n",
    "   $C[2] = A[0] \\cdot B[2] + A[1] \\cdot B[1] + A[2] \\cdot B[0]$\n",
    "   $C[2] = 3 \\cdot 3 + 4 \\cdot 2 + 5 \\cdot 1 = 9 + 8 + 5 = 22$\n",
    "4. **$C[3]$**\n",
    "   $C[3] = A[1] \\cdot B[2] + A[2] \\cdot B[1] + A[3] \\cdot B[0]$\n",
    "   $C[3] = 4 \\cdot 3 + 5 \\cdot 2 + 6 \\cdot 1 = 12 + 10 + 6 = 28$\n",
    "5. **$C[4]$**\n",
    "   $C[4] = A[2] \\cdot B[2] + A[3] \\cdot B[1] + A[4] \\cdot B[0]$\n",
    "   $C[4] = 5 \\cdot 3 + 6 \\cdot 2 + 2 \\cdot 1 = 15 + 12 + 2 = 29$\n",
    "6. **\\( C[5] \\)**\n",
    "   $C[5] = A[3] \\cdot B[2] + A[4] \\cdot B[1]$\n",
    "   $C[5] = 6 \\cdot 3 + 2 \\cdot 2 = 18 + 4 = 22$\n",
    "7. **$C[6]$**\n",
    "   $C[6] = A[4] \\cdot B[2] = 2 \\cdot 3 = 6$\n",
    "---\n",
    "### **Result**\n",
    "$C = [3, 10, 22, 28, 29, 22, 6]$\n",
    "\n",
    "---\n",
    "#### **How convolution changes the original data**\n",
    "first, convolution operator result of will always be increased in length comparing to the original arrays, following the rule: ($sum[lengths of arrays]-1$). **In signal processing**, convolution is often used to smooth singals because its combining neighboring points of one signal in accordance with another one (usually certain kernel). in signal processing its often usesd to enhance noticeable and sudden changes within a signal and help detect spikes and increase SNR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Answer2-FFT vs Convolution**\n",
    "FFT key concept is moving from the time domain to the frequency domain. Of note,convolution in the time domain is equivalent to multiplication in the frequency domain. when comparing computetional complexity-unlike direct convolution with a complexity of $O(N^2)$, FFT-based convolution operates at $O(N \\log N)$. the complexity of computation affect the time it takes to make all the calculations. In neuroscience,electrphysiological recordings ususally consist >1000 of time points (depends on sampling rate of the signal) and/or multichannel recordings. Running time or speed of calculations is critical for real-time applications usually that are usually handy in electropysiological recordings. By working in the frequency domain, it allows efficient filtering of neural signals to isolate specific frequency bands (delta, theta, alpha, beta, gamma) and avoids the large memory overhead of direct convolution. It also enables parallel processing of multiple signal channels, scales well for large datasets, and supports the design of custom filters for noise reduction, smoothing, and event detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical expression for the FFT-based convolution of **$X[n]$ and $Y[n]$** where $n=1000$ is:  \n",
    "$C[n] = \\text{IFFT} \\left( \\text{FFT}(X[n], L) \\cdot \\text{FFT}(Y[n], L) \\right)$\n",
    "Where $L =1000+1000-1=1999$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Answer3-Moving average signal** \n",
    "- We have the following signal:  \n",
    "S = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- moving average of this signal using FIR with a window size $W = 3$.  \n",
    "The filter coefficients for a window size  $W$ are:  \n",
    "$ h = \\left[ \\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3} \\right]$\n",
    "\n",
    "- The window size $W = 3$ means we slide a window of size 3 over the signal $S$.  \n",
    "- The output signal $y[n]$ will have a length of:  \n",
    "\n",
    "  $N_{\\text{out}} = N_{\\text{in}} - W + 1$\n",
    "  $For $N_{\\text{in}} = 9$ and $W = 3$:  \n",
    "  $N_{\\text{out}} = 9 - 3 + 1 = 7$\n",
    "\n",
    "---\n",
    "\n",
    "### **Calculation**\n",
    "We now calculate the moving average for each position.  \n",
    "\n",
    "-  $[2, 3, 4]$ |  $y[0] = \\frac{1}{3}(2 + 3 + 4) = \\frac{9}{3} = 3$\n",
    "\n",
    "-  $[3, 4, 5]$ | $y[1] = \\frac{1}{3}(3 + 4 + 5) = \\frac{12}{3} = 4$\n",
    "\n",
    "- $[4, 5, 6]$  | $y[2] = \\frac{1}{3}(4 + 5 + 6) = \\frac{15}{3} = 5$\n",
    "\n",
    "- $[5, 6, 7]$  | $y[3] = \\frac{1}{3}(5 + 6 + 7) = \\frac{18}{3} = 6$\n",
    "\n",
    "- $[6, 7, 8]$  | $y[4] = \\frac{1}{3}(6 + 7 + 8) = \\frac{21}{3} = 7$\n",
    "\n",
    "- $[7, 8, 9]$  | $y[5] = \\frac{1}{3}(7 + 8 + 9) = \\frac{24}{3} = 8$\n",
    "\n",
    "- $[8, 9, 10]$ | $y[6] = \\frac{1}{3}(8 + 9 + 10) = \\frac{27}{3} = 9$\n",
    "\n",
    "\n",
    "## **Final Result** \n",
    "$y = [3, 4, 5, 6, 7, 8, 9]$\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A moving average filter reduce high fluctuations and enhance overall trends in a signal (smoothing the signal). It does this by averaging groups of three consecutive points(in our example), which helps to minimize the impact of random fluctuations or noise. with that said, this process introduces a slight delay (latency) in the signal since the filter requires future points to compute the average. For a window size of 3, the delay is approximately 1 sample, meaning the filtered output is slightly behind the original signal. This delay is an important consideration in when executing real-time analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **programming Exercises**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show using Python's `numpy.convolve`that convolution is a commutative operator.**\n",
    "\n",
    "A commutative operator is an operator that gives the same result regardless of the order of the operands. \n",
    "an operator is commutative if:\n",
    "- $ ùê¥‚àóùêµ=ùêµ‚àóùê¥ $\n",
    "- $  A‚àóB=B‚àóA $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#two arrays\n",
    "A = np.array([1, 2, 3])\n",
    "B = np.array([4, 5, 6])\n",
    "\n",
    "# convolution of A * B\n",
    "A_B = np.convolve(A, B, mode='full')\n",
    "\n",
    "# convolution of B * A\n",
    "B_A = np.convolve(B,A, mode='full')\n",
    "\n",
    "# Display the results\n",
    "A_B, B_A, np.array_equal(A_B, B_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the built-in convolve function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convolveBuiltIn(HB1, HB2):\n",
    "    HB1 = np.array(HB1)\n",
    "    HB2 = np.array(HB2)\n",
    "    \n",
    "    #built-in function\n",
    "    convolved_array = np.convolve(HB1, HB2, mode='full')\n",
    "    \n",
    "    return convolved_array\n",
    "\n",
    "HB1 = [1, 5, 7, 9, 3, 3, 3, 9, 8] \n",
    "HB2 = [1, 3]\n",
    "\n",
    "# Call the function and print the result\n",
    "result = convolveBuiltIn (HB1, HB2)\n",
    "print(\"Convolved Array:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My own convolution operator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convolveHB(HB1, HB2):\n",
    "    # Convert input lists to NumPy arrays\n",
    "    HB1 = np.array(HB1)  # Signal\n",
    "    HB2 = np.array(HB2)  # Kernel\n",
    "\n",
    "    # Flip the kernel\n",
    "    HB2 = np.flip(HB2)\n",
    "\n",
    "    # Get the lengths of the signal and kernel\n",
    "    HB1_length = len(HB1)\n",
    "    HB2_length = len(HB2)\n",
    "\n",
    "    # Calculate the size of padding\n",
    "    pad_size = HB2_length - 1  \n",
    "    padded_sig = np.pad(HB1, (pad_size, pad_size), mode='constant', constant_values=0)\n",
    "\n",
    "    # Size of the array after convolution\n",
    "    convolved_length = HB1_length + HB2_length - 1\n",
    "\n",
    "    # Empty output array to store the convolution result\n",
    "    convolved_array = np.zeros(convolved_length) \n",
    "\n",
    "    # Slide the kernel over the signal, multiply and sum\n",
    "    for i in range(convolved_length):\n",
    "        region = padded_sig[i:i + HB2_length]\n",
    "        convolved_array[i] = np.sum(region * HB2) \n",
    "\n",
    "    return convolved_array\n",
    "\n",
    "\n",
    "HB1 = [1, 5, 7, 9, 3, 3, 3, 9, 8] \n",
    "HB2 = [1, 3]\n",
    "\n",
    "result = convolveHB(HB1, HB2)\n",
    "print(\"Convolved Array:\", result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loading and plotting EEG signals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load EEG recordings from 'eeg_signals.pkl'\n",
    "with open('eeg_signals.pkl', 'rb') as file:\n",
    "    eeg_signals = pickle.load(file)\n",
    "\n",
    "# Function to visualize EEG signals in the time domain\n",
    "def plot_time_domain(eeg_signals):\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for i, signal in enumerate(eeg_signals):\n",
    "        plt.subplot(10, 1, i + 1)\n",
    "        plt.plot(signal, color='orange', label=f'Signal {i + 1}')\n",
    "        plt.xlabel('Time (samples)')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.title(f'EEG Signal {i + 1} in Time Domain')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize EEG signals in the frequency domain using traditional FFT (without assuming sampling rate)\n",
    "def plot_frequency_domain_no_assumption(eeg_signals):\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for i, signal in enumerate(eeg_signals):\n",
    "        n = len(signal)\n",
    "        if n % 2 != 0:\n",
    "            n -= 1  # Ensure n is even to avoid slicing errors\n",
    "        f_norm = np.fft.fftfreq(n)[:n // 2]  # Normalized frequency (0 to 0.5)\n",
    "        fft_values = np.abs(np.fft.fft(signal)[:n // 2])\n",
    "        plt.subplot(10, 1, i + 1)\n",
    "        plt.plot(f_norm, fft_values, color='pink', label=f'Signal {i + 1}')\n",
    "        plt.xlabel('Normalized Frequency (cycles/sample)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.title(f'EEG Signal {i + 1} in Frequency Domain (Normalized Frequency)')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to visualize EEG signals in the frequency domain using traditional FFT (with assumed sampling rate)\n",
    "def plot_frequency_domain_with_assumption(eeg_signals, fs=1000):  # Assuming a sampling frequency of 1000 Hz\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    for i, signal in enumerate(eeg_signals):\n",
    "        n = len(signal)\n",
    "        if n % 2 != 0:\n",
    "            n -= 1  # Ensure n is even to avoid slicing errors\n",
    "        f = np.fft.fftfreq(n, d=1/fs)[:n // 2]  # Frequency in Hz\n",
    "        fft_values = np.abs(np.fft.fft(signal)[:n // 2])\n",
    "        plt.subplot(10, 1, i + 1)\n",
    "        plt.plot(f, fft_values, color='green', label=f'Signal {i + 1}')\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.title(f'EEG Signal {i + 1} in Frequency Domain (With Assumed Sampling Rate)')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot the data\n",
    "plot_time_domain(eeg_signals)\n",
    "plot_frequency_domain_no_assumption(eeg_signals)\n",
    "plot_frequency_domain_with_assumption(eeg_signals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explenation of differences betweeen time and frequency domains\n",
    "\n",
    "**creating ERP template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from the .pkl file\n",
    "with open('eeg_signals.pkl', 'rb') as file:\n",
    "    eeg_signals = pickle.load(file)\n",
    "\n",
    "# ERP timestamps from a .txt file\n",
    "erp_timestamps = np.loadtxt('erp_timestamps.txt')\n",
    "\n",
    "# Align the EEG signals based on ERP timestamps\n",
    "def align_eeg_to_erp(eeg_data, erp_timestamps):\n",
    "    \"\"\"\n",
    "    Align EEG data based on ERP timestamps.\n",
    "    \n",
    "    Args:\n",
    "        eeg_data (list of np.array): List of EEG recordings.\n",
    "        erp_timestamps (list of float): List of ERP timestamps (in seconds) corresponding to each recording.\n",
    "    \n",
    "    Returns:\n",
    "        aligned_data (np.array): Aligned EEG segments for each trial.\n",
    "    \"\"\"\n",
    "    aligned_data = []\n",
    "    for i, signal in enumerate(eeg_data):\n",
    "        event_index = int(erp_timestamps[i])  # Use the ERP timestamp directly as an index\n",
    "        start_index = 0  # Start from the beginning of signal\n",
    "        end_index = len(signal)  # End at the end of signal\n",
    "        \n",
    "        if end_index > start_index:\n",
    "            aligned_segment = signal[start_index:end_index]\n",
    "            aligned_data.append(aligned_segment)\n",
    "    \n",
    "    return np.array(aligned_data)\n",
    "\n",
    "# Create an average ERP template\n",
    "def compute_average_erp(aligned_data):\n",
    "    \"\"\"\n",
    "    Compute the average ERP from aligned EEG data.\n",
    "    \n",
    "    Args:\n",
    "        aligned_data (np.array): 2D array where each row is an aligned EEG segment.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The average ERP waveform.\n",
    "    \"\"\"\n",
    "    return np.mean(aligned_data, axis=0)\n",
    "\n",
    "# Plot the average ERP\n",
    "def plot_erp(average_erp):\n",
    "    \"\"\"\n",
    "    Plot the averaged ERP signal.\n",
    "    \n",
    "    Args:\n",
    "        average_erp (np.array): The average ERP waveform to plot.\n",
    "    \"\"\"\n",
    "    time_axis = np.linspace(0, len(average_erp), len(average_erp))  # Time in samples\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(time_axis, average_erp, label='Average ERP', color='purple')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Amplitude (uV)')\n",
    "    plt.title('Averaged Event-Related Potential (ERP)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to execute the entire process\n",
    "def main():\n",
    "    #EEG data from file\n",
    "    eeg_data = eeg_signals\n",
    "    \n",
    "    #ERP timestamps from text file\n",
    "    erp_timestamps = np.loadtxt('erp_timestamps.txt')\n",
    "    \n",
    "    #Align EEG to ERP timestamps\n",
    "    aligned_data = align_eeg_to_erp(eeg_data, erp_timestamps)\n",
    "    \n",
    "    #average ERP template\n",
    "    average_erp = compute_average_erp(aligned_data)\n",
    "    \n",
    "    #Plot the averaged ERP\n",
    "    plot_erp(average_erp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matched Filter Convolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load EEG recordings from 'eeg_signals.pkl'\n",
    "with open('eeg_signals.pkl', 'rb') as file:\n",
    "    eeg_signals = pickle.load(file)\n",
    "\n",
    "# Assuming ERP template is the first signal in eeg_signals for demonstration purposes\n",
    "erp_template = eeg_signals[0]  \n",
    "\n",
    "# Define new EEG recording (where ERP location is unknown)\n",
    "new_eeg_recording = eeg_signals[1]  \n",
    "\n",
    "# Perform convultion from my original function\n",
    "convolution_result = convolveHB(new_eeg_recording, erp_template)\n",
    "\n",
    "# Identify the location where the ERP is most likely to occur\n",
    "max_location = np.argmax(convolution_result)  # Index of the maximum value in the convolution result\n",
    "\n",
    "# Plot the convolution result and indicate the location of the most likely ERP\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(convolution_result, color='navy', label='Convolution Output')\n",
    "plt.axvline(x=max_location, color='cyan', linestyle='--', label=f'Most Likely ERP Location (Index: {max_location})')\n",
    "plt.xlabel('Time (samples)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Convolution Result of EEG Recording with ERP Template')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the location of the most likely ERP occurrence\n",
    "print(f\"The most likely location of the ERP in the new EEG recording is at index: {max_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dicussion of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Poisson Spike Trains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data from file (readPoisSpikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat  # Import at the top of the file\n",
    "\n",
    "def readPoiSpikes(fileName, bin_width):\n",
    "    \"\"\"\n",
    "    readPoiSpikes - Loads a spike train dataset from a file and outputs it as a spike train with a set bin width.\n",
    "    \n",
    "    Input Arguments:\n",
    "    fileName (str): The path to the file containing the spike train data.\n",
    "    bin_width (float): The width of the bins in seconds to discretize the spike train.\n",
    "    r = 94 #spikes per second\n",
    "    Fs = 1e3 #Sampling frequency\n",
    "    totalTime = 30 #seconds\n",
    "    dt = 0.001 #seconds\n",
    "    binSize = 0.01 #seconds\n",
    "    Returns:\n",
    "    np.ndarray: A binned spike train where each bin is 1 if a spike occurred in that bin and 0 otherwise.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    try:\n",
    "        with open(fileName, 'r') as file:\n",
    "            pass\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File does not exist: {fileName}\")\n",
    "    \n",
    "    # Load the .mat file and extract spike train data\n",
    "    try:\n",
    "        mat_data = loadmat(fileName)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load .mat file. Error: {str(e)}\")\n",
    "    \n",
    "    # Extract the first variable containing the spike times\n",
    "    spike_times = None\n",
    "    for key in mat_data:\n",
    "        if not key.startswith(\"__\") and isinstance(mat_data[key], np.ndarray):\n",
    "            spike_times = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    if spike_times is None:\n",
    "        raise ValueError(\"No valid spike time array found in the .mat file.\")\n",
    "    \n",
    "    # Check if spike_times is a row or column vector and convert to 1D array\n",
    "    spike_times = np.ravel(spike_times)\n",
    "    \n",
    "    # Validate the spike_times data\n",
    "    if not np.issubdtype(spike_times.dtype, np.number):\n",
    "        raise ValueError(\"must be a numeric vector\")\n",
    "    \n",
    "    # Handle the case of an empty spike_times array\n",
    "    if spike_times.size == 0:\n",
    "        raise ValueError(\"no data in file\")\n",
    "    \n",
    "    # Remove negative spike times, if any\n",
    "    spike_times = spike_times[spike_times >= 0]\n",
    "    \n",
    "    # Determine the maximum spike time to define the duration of the spike train\n",
    "    max_time = np.max(spike_times)\n",
    "    \n",
    "    # Calculate the number of bins required for the given bin width\n",
    "    num_bins = int(np.ceil(max_time / bin_width))\n",
    "    \n",
    "    # Create the binned spike train as an array of zeros\n",
    "    spike_train = np.zeros(num_bins, dtype=bool)\n",
    "    \n",
    "    # Convert spike times to bin indices\n",
    "    bin_indices = np.ceil(spike_times / bin_width).astype(int) - 1  # Convert to 0-indexed\n",
    "    \n",
    "    # Ensure bin indices are within range\n",
    "    bin_indices = bin_indices[(bin_indices >= 0) & (bin_indices < num_bins)]\n",
    "    \n",
    "    # Mark the bins where spikes occurred\n",
    "    spike_train[bin_indices] = True\n",
    "    \n",
    "    return spike_train\n",
    "\n",
    "\n",
    "#call for the function#\n",
    "file_path = 'C:\\Users\\halit\\OneDrive\\Documents\\GitHub\\BSA-2024\\week-3-poisson-continued/rawSpikes1.mat'  #path to your .mat file\n",
    "bin_width = 0.001  # Example bin width of 1 ms\n",
    "\n",
    "spike_train = readPoiSpikes(file_path, bin_width)\n",
    "print(\"Binned Spike Train:\", spike_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Spike Trains (generatePoiSpikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generatePoiSpikes(r, dt, totalTime):\n",
    "    \"\"\"\n",
    "    generatePoiSpikes - Generates a spike train according to the Poisson process model.\n",
    "    \n",
    "    Input Arguments:\n",
    "    r (float): Rate parameter (spikes per second) of the homogeneous Poisson process.\n",
    "    dt (float): Time step (in seconds) of the discretized spike train.\n",
    "    totalTime (float): Total time (in seconds) for which the spike train is generated.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: A binary spike train where each bin is 1 if a spike occurred and 0 otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    totalSize = int(totalTime / dt)\n",
    "    \n",
    "    random_numbers = np.random.rand(totalSize)\n",
    "    \n",
    "    spike_prob = r * dt\n",
    "    \n",
    "    spike_train = random_numbers < spike_prob\n",
    "    \n",
    "    # Plot the spike train\n",
    "    time_vector = np.linspace(0, totalTime, totalSize, endpoint=False)  # Ensuring proper length and no off-by-one errors\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.step(time_vector, spike_train, where='post', color='red')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Spikes (0 or 1)')\n",
    "    plt.title('Generated Poisson Spike Train')\n",
    "    plt.show()\n",
    "    \n",
    "    return spike_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fano Factor and Coefficient of Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**calcFF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcFF(spike_train):\n",
    "\n",
    "    # Determine the window size as the configured bin size (number of samples per window)\n",
    "    window_size = len(spike_train) // 30  # Assuming 30-second total time\n",
    "    \n",
    "    # Divide the spike train into non-overlapping windows of the given size\n",
    "    num_windows = len(spike_train) // window_size\n",
    "    spike_counts = np.array([np.sum(spike_train[i * window_size:(i + 1) * window_size]) for i in range(num_windows)])\n",
    "    \n",
    "    # Calculate Fano Factor as variance / mean\n",
    "    FF = np.var(spike_counts) / np.mean(spike_counts) if np.mean(spike_counts) > 0 else np.nan\n",
    "    \n",
    "    # Determine if the neuron is homogeneous\n",
    "    if FF > 1:\n",
    "        print(\"The neuron is not homogeneous.\")\n",
    "    else:\n",
    "        print(\"The neuron is homogeneous.\")\n",
    "    \n",
    "    return FF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**calcCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calcCV(spike_train):\n",
    "    # Extract spike times from the binary spike train\n",
    "    spike_times = np.where(spike_train == 1)[0]  # Indices of spikes\n",
    "    \n",
    "    # Calculate Inter-Spike Intervals (ISIs)\n",
    "    if len(spike_times) < 2:\n",
    "        return np.nan  # Return NaN if there are fewer than 2 spikes\n",
    "    \n",
    "    isis = np.diff(spike_times)  # Calculate differences between successive spike times\n",
    "    \n",
    "    # Calculate Coefficient of Variation (CV) as std / mean\n",
    "    CV = np.std(isis) / np.mean(isis) if np.mean(isis) > 0 else np.nan\n",
    "    \n",
    "    # Determine if the neuron is homogeneous\n",
    "    if CV > 1:\n",
    "        print(\"The neuron is not homogeneous.\")\n",
    "    else:\n",
    "        print(\"The neuron is homogeneous.\")\n",
    "    \n",
    "    return CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**calcRate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcRate(spike_train, window, dt):\n",
    "    \"\"\"\n",
    "    Calculates the rate of fire of a neuron from a spike train.\n",
    "    \n",
    "    Input Arguments:\n",
    "    spike_train (np.ndarray): The binary spike train where each bin is 1 if a spike occurred and 0 otherwise.\n",
    "    window (float): The length of the window in seconds to calculate the firing rate. If window = 0, calculate the average firing rate.\n",
    "    dt (float): The time step (in seconds) of the spike train.\n",
    "    \n",
    "    Returns:\n",
    "    float or np.ndarray: The firing rate over the entire train or the firing rate over time if a window is specified.\n",
    "    \"\"\"\n",
    "    if window == 0:\n",
    "        # Calculate the overall rate of fire\n",
    "        total_spikes = np.sum(spike_train)\n",
    "        total_time = len(spike_train) * dt\n",
    "        RateOfFire = total_spikes / total_time\n",
    "    else:\n",
    "        # Calculate the rate of fire in the specified window\n",
    "        window_size = int(window / dt)  # Convert window size from seconds to number of bins\n",
    "        num_windows = len(spike_train) // window_size\n",
    "        RateofFire = np.array([np.sum(spike_train[i * window_size:(i + 1) * window_size]) / (window * dt) for i in range(num_windows)])\n",
    "        \n",
    "        # Plot the rate of fire over time\n",
    "        time_points = np.arange(0, len(rate_of_fire)) * window  # Times corresponding to the start of each window\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(time_points, rate_of_fire, label='Rate of Fire', color='purple')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Firing Rate (spikes/s)')\n",
    "        plt.title('Rate of Fire Over Time')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return RateOfFire\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
